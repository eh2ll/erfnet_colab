{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ERFNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7mlftzxcz9KC"},"source":["## Connect Google Drive\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duYk_Mjs1Nnf","executionInfo":{"status":"ok","timestamp":1614610908558,"user_tz":-540,"elapsed":27574,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}},"outputId":"ac39e3ed-28ce-44c9-9313-dbe6d07f87bb"},"source":["! npm install -g localtunnel\r\n","# 8097 is the port number I set myself, which can be modified to the port number I want to use\r\n","get_ipython().system_raw('python3 -m pip install visdom')\r\n","get_ipython().system_raw('python3 -m visdom.server -port 8097 >> visdomlog.txt 2>&1 &')   \r\n","get_ipython().system_raw('lt --port 8097 >> url.txt 2>&1 &')   \r\n","import time\r\n","time.sleep(5)\r\n","! cat url.txt\r\n","import visdom\r\n","time.sleep(5)\r\n","vis = visdom.Visdom(port='8097')  \r\n","print(vis)\r\n","time.sleep(3)\r\n","vis.text('testing')\r\n","! cat visdomlog.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n","+ localtunnel@2.0.1\n","added 22 packages from 22 contributors in 1.387s\n","your url is: https://tasty-bullfrog-59.loca.lt\n"],"name":"stdout"},{"output_type":"stream","text":["Setting up a new session...\n"],"name":"stderr"},{"output_type":"stream","text":["<visdom.Visdom object at 0x7f6c53cde650>\n","/usr/local/lib/python3.7/dist-packages/visdom/server.py:39: DeprecationWarning: zmq.eventloop.ioloop is deprecated in pyzmq 17. pyzmq now works with default tornado and asyncio eventloops.\n","  ioloop.install()  # Needs to happen before any tornado imports!\n","INFO:root:Application Started\n","INFO:tornado.access:200 POST /env/main (127.0.0.1) 0.58ms\n","INFO:tornado.access:101 GET /vis_socket (127.0.0.1) 0.41ms\n","INFO:root:Opened visdom socket from ip: 127.0.0.1\n","INFO:tornado.access:200 GET / (127.0.0.1) 14.90ms\n","INFO:tornado.access:200 GET /static/css/react-resizable-styles.css?v=9f91a8dbf4d8f7ef1399e625660405f4 (127.0.0.1) 1.03ms\n","INFO:tornado.access:200 GET /static/css/bootstrap.min.css?v=ec3bb52a00e176a7181d454dffaea219 (127.0.0.1) 5.14ms\n","INFO:tornado.access:200 GET /static/css/react-grid-layout-styles.css?v=7dc8934d2f9ac5303b8f0bb1148152a0 (127.0.0.1) 0.71ms\n","INFO:tornado.access:200 GET /static/css/style.css?v=fb4729780f2360ef5d3dba13f8ff2873 (127.0.0.1) 0.63ms\n","INFO:tornado.access:200 GET /static/js/jquery.min.js?v=e071abda8fe61194711cfc2ab99fe104 (127.0.0.1) 1.31ms\n","INFO:tornado.access:200 GET /static/js/bootstrap.min.js?v=5869c96cc8f19086aee625d670d741f9 (127.0.0.1) 0.81ms\n","INFO:tornado.access:200 GET /static/js/react-react.min.js?v=bca103da5b5404d93783ccf73e0e9d1e (127.0.0.1) 0.67ms\n","INFO:tornado.access:200 GET /static/js/layout_bin_packer.js?v=6c46683ed70fbb1443caf3531243836d (127.0.0.1) 0.66ms\n","INFO:tornado.access:200 GET /static/js/react-dom.min.js?v=950495cc51ccb90612cf0fe0bb44f8f3 (127.0.0.1) 10.63ms\n","INFO:tornado.access:200 GET /static/js/plotly-plotly.min.js?v=059f6ecfa3930aa0b85b6ef4591390e4 (127.0.0.1) 101.95ms\n","INFO:tornado.access:200 GET /static/js/main.js?v=47fc3afa811b358b354df09184e392e7 (127.0.0.1) 9.78ms\n","INFO:tornado.access:200 POST /events (127.0.0.1) 0.65ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpSK-R8rz86x","executionInfo":{"status":"ok","timestamp":1614610935139,"user_tz":-540,"elapsed":54121,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}},"outputId":"96dcedea-1a38-414d-a399-f805ad34c19c"},"source":["from google.colab import drive \r\n","\r\n","drive.mount('/content/gdrive/')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EvdmPAHwx4Hv"},"source":["## Load cityscapes dataset"]},{"cell_type":"code","metadata":{"id":"5c4Y2qVhw4Iy","executionInfo":{"status":"ok","timestamp":1614610935143,"user_tz":-540,"elapsed":54110,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["import numpy as np\r\n","import os\r\n","\r\n","from PIL import Image\r\n","\r\n","from torch.utils.data import Dataset\r\n","\r\n","EXTENSIONS = ['.jpg', '.png']\r\n","\r\n","def load_image(file):\r\n","    return Image.open(file)\r\n","\r\n","def is_image(filename):\r\n","    return any(filename.endswith(ext) for ext in EXTENSIONS)\r\n","\r\n","def is_label(filename):\r\n","    return filename.endswith(\"_labelTrainIds.png\")\r\n","\r\n","def image_path(root, basename, extension):\r\n","    return os.path.join(root, f'{basename}{extension}')\r\n","\r\n","def image_path_city(root, name):\r\n","    return os.path.join(root, f'{name}')\r\n","\r\n","def image_basename(filename):\r\n","    return os.path.basename(os.path.splitext(filename)[0])\r\n","\r\n","class VOC12(Dataset):\r\n","\r\n","    def __init__(self, root, input_transform=None, target_transform=None):\r\n","        self.images_root = os.path.join(root, 'images')\r\n","        self.labels_root = os.path.join(root, 'labels')\r\n","\r\n","        self.filenames = [image_basename(f)\r\n","            for f in os.listdir(self.labels_root) if is_image(f)]\r\n","        self.filenames.sort()\r\n","\r\n","        self.input_transform = input_transform\r\n","        self.target_transform = target_transform\r\n","\r\n","    def __getitem__(self, index):\r\n","        filename = self.filenames[index]\r\n","\r\n","        with open(image_path(self.images_root, filename, '.jpg'), 'rb') as f:\r\n","            image = load_image(f).convert('RGB')\r\n","        with open(image_path(self.labels_root, filename, '.png'), 'rb') as f:\r\n","            label = load_image(f).convert('P')\r\n","\r\n","        if self.input_transform is not None:\r\n","            image = self.input_transform(image)\r\n","        if self.target_transform is not None:\r\n","            label = self.target_transform(label)\r\n","\r\n","        return image, label\r\n","\r\n","    def __len__(self):\r\n","        return len(self.filenames)\r\n","\r\n","\r\n","\r\n","\r\n","class cityscapes(Dataset):\r\n","\r\n","    def __init__(self, root, co_transform=None, subset='train'):\r\n","        self.images_root = os.path.join(root, 'leftImg8bit/')\r\n","        self.labels_root = os.path.join(root, 'gtFine/')\r\n","        \r\n","        self.images_root += subset\r\n","        self.labels_root += subset\r\n","\r\n","        print (self.images_root)\r\n","        #self.filenames = [image_basename(f) for f in os.listdir(self.images_root) if is_image(f)]\r\n","        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\r\n","        self.filenames.sort()\r\n","\r\n","        #[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(\".\")) for f in fn]\r\n","        #self.filenamesGt = [image_basename(f) for f in os.listdir(self.labels_root) if is_image(f)]\r\n","        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\r\n","        self.filenamesGt.sort()\r\n","\r\n","        self.co_transform = co_transform # ADDED THIS\r\n","\r\n","\r\n","    def __getitem__(self, index):\r\n","        filename = self.filenames[index]\r\n","        filenameGt = self.filenamesGt[index]\r\n","\r\n","        with open(image_path_city(self.images_root, filename), 'rb') as f:\r\n","            image = load_image(f).convert('RGB')\r\n","        with open(image_path_city(self.labels_root, filenameGt), 'rb') as f:\r\n","            label = load_image(f).convert('P')\r\n","\r\n","        if self.co_transform is not None:\r\n","            image, label = self.co_transform(image, label)\r\n","\r\n","        return image, label\r\n","\r\n","    def __len__(self):\r\n","        return len(self.filenames)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7dIBPw72yGdh"},"source":["## ERFNet model definition"]},{"cell_type":"code","metadata":{"id":"rIRGLl2HQKCB","executionInfo":{"status":"ok","timestamp":1614610935565,"user_tz":-540,"elapsed":54523,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["# ERFNet full model definition for Pytorch\r\n","# Sept 2017\r\n","# Eduardo Romera\r\n","#######################\r\n","\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.init as init\r\n","import torch.nn.functional as F\r\n","\r\n","class DownsamplerBlock (nn.Module):\r\n","    def __init__(self, ninput, noutput):\r\n","        super().__init__()\r\n","\r\n","        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\r\n","        self.pool = nn.MaxPool2d(2, stride=2)\r\n","        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\r\n","\r\n","    def forward(self, input):\r\n","        output = torch.cat([self.conv(input), self.pool(input)], 1)\r\n","        output = self.bn(output)\r\n","        return F.relu(output)\r\n","    \r\n","\r\n","class non_bottleneck_1d (nn.Module):\r\n","    def __init__(self, chann, dropprob, dilated):        \r\n","        super().__init__()\r\n","\r\n","        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\r\n","\r\n","        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\r\n","\r\n","        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\r\n","\r\n","        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\r\n","\r\n","        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\r\n","\r\n","        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\r\n","\r\n","        self.dropout = nn.Dropout2d(dropprob)\r\n","        \r\n","\r\n","    def forward(self, input):\r\n","\r\n","        output = self.conv3x1_1(input)\r\n","        output = F.relu(output)\r\n","        output = self.conv1x3_1(output)\r\n","        output = self.bn1(output)\r\n","        output = F.relu(output)\r\n","\r\n","        output = self.conv3x1_2(output)\r\n","        output = F.relu(output)\r\n","        output = self.conv1x3_2(output)\r\n","        output = self.bn2(output)\r\n","\r\n","        if (self.dropout.p != 0):\r\n","            output = self.dropout(output)\r\n","        \r\n","        return F.relu(output+input)    #+input = identity (residual connection)\r\n","\r\n","\r\n","class Encoder(nn.Module):\r\n","    def __init__(self, num_classes):\r\n","        super().__init__()\r\n","        self.initial_block = DownsamplerBlock(3,16)\r\n","\r\n","        self.layers = nn.ModuleList()\r\n","\r\n","        self.layers.append(DownsamplerBlock(16,64))\r\n","\r\n","        for x in range(0, 5):    #5 times\r\n","           self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \r\n","\r\n","        self.layers.append(DownsamplerBlock(64,128))\r\n","\r\n","        for x in range(0, 2):    #2 times\r\n","            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\r\n","            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\r\n","            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\r\n","            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\r\n","\r\n","        #Only in encoder mode:\r\n","        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\r\n","\r\n","    def forward(self, input, predict=False):\r\n","        output = self.initial_block(input)\r\n","\r\n","        for layer in self.layers:\r\n","            output = layer(output)\r\n","\r\n","        if predict:\r\n","            output = self.output_conv(output)\r\n","\r\n","        return output\r\n","\r\n","\r\n","class UpsamplerBlock (nn.Module):\r\n","    def __init__(self, ninput, noutput):\r\n","        super().__init__()\r\n","        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\r\n","        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\r\n","\r\n","    def forward(self, input):\r\n","        output = self.conv(input)\r\n","        output = self.bn(output)\r\n","        return F.relu(output)\r\n","\r\n","class Decoder (nn.Module):\r\n","    def __init__(self, num_classes):\r\n","        super().__init__()\r\n","\r\n","        self.layers = nn.ModuleList()\r\n","\r\n","        self.layers.append(UpsamplerBlock(128,64))\r\n","        self.layers.append(non_bottleneck_1d(64, 0, 1))\r\n","        self.layers.append(non_bottleneck_1d(64, 0, 1))\r\n","\r\n","        self.layers.append(UpsamplerBlock(64,16))\r\n","        self.layers.append(non_bottleneck_1d(16, 0, 1))\r\n","        self.layers.append(non_bottleneck_1d(16, 0, 1))\r\n","\r\n","        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\r\n","\r\n","    def forward(self, input):\r\n","        output = input\r\n","\r\n","        for layer in self.layers:\r\n","            output = layer(output)\r\n","\r\n","        output = self.output_conv(output)\r\n","\r\n","        return output\r\n","\r\n","#ERFNet\r\n","class Net(nn.Module):\r\n","    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\r\n","        super().__init__()\r\n","\r\n","        if (encoder == None):\r\n","            self.encoder = Encoder(num_classes)\r\n","        else:\r\n","            self.encoder = encoder\r\n","        self.decoder = Decoder(num_classes)\r\n","\r\n","    def forward(self, input, only_encode=False):\r\n","        if only_encode:\r\n","            return self.encoder.forward(input, predict=True)\r\n","        else:\r\n","            output = self.encoder(input)    #predict=False by default\r\n","            return self.decoder.forward(output)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4qUd0BYzy3p"},"source":["## Calculate IoU on each epoch during training"]},{"cell_type":"code","metadata":{"id":"mtvDwB-syAJD","executionInfo":{"status":"ok","timestamp":1614610935570,"user_tz":-540,"elapsed":54519,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["import torch\r\n","\r\n","class iouEval:\r\n","\r\n","    def __init__(self, nClasses, ignoreIndex=19):\r\n","        self.nClasses = nClasses\r\n","        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\r\n","        self.reset()\r\n","\r\n","    def reset (self):\r\n","        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\r\n","        self.tp = torch.zeros(classes).double()\r\n","        self.fp = torch.zeros(classes).double()\r\n","        self.fn = torch.zeros(classes).double()        \r\n","\r\n","    def addBatch(self, x, y):   #x=preds, y=targets\r\n","        #sizes should be \"batch_size x nClasses x H x W\"\r\n","        \r\n","        #print (\"X is cuda: \", x.is_cuda)\r\n","        #print (\"Y is cuda: \", y.is_cuda)\r\n","\r\n","        if (x.is_cuda or y.is_cuda):\r\n","            x = x.cuda()\r\n","            y = y.cuda()\r\n","\r\n","        #if size is \"batch_size x 1 x H x W\" scatter to onehot\r\n","        if (x.size(1) == 1):\r\n","            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \r\n","            if x.is_cuda:\r\n","                x_onehot = x_onehot.cuda()\r\n","            x_onehot.scatter_(1, x, 1).float()\r\n","        else:\r\n","            x_onehot = x.float()\r\n","\r\n","        if (y.size(1) == 1):\r\n","            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\r\n","            if y.is_cuda:\r\n","                y_onehot = y_onehot.cuda()\r\n","            y_onehot.scatter_(1, y, 1).float()\r\n","        else:\r\n","            y_onehot = y.float()\r\n","\r\n","        if (self.ignoreIndex != -1): \r\n","            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\r\n","            x_onehot = x_onehot[:, :self.ignoreIndex]\r\n","            y_onehot = y_onehot[:, :self.ignoreIndex]\r\n","        else:\r\n","            ignores=0\r\n","\r\n","        #print(type(x_onehot))\r\n","        #print(type(y_onehot))\r\n","        #print(x_onehot.size())\r\n","        #print(y_onehot.size())\r\n","\r\n","        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\r\n","        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\r\n","        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\r\n","        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\r\n","        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\r\n","        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \r\n","\r\n","        self.tp += tp.double().cpu()\r\n","        self.fp += fp.double().cpu()\r\n","        self.fn += fn.double().cpu()\r\n","\r\n","    def getIoU(self):\r\n","        num = self.tp\r\n","        den = self.tp + self.fp + self.fn + 1e-15\r\n","        iou = num / den\r\n","        return torch.mean(iou), iou     #returns \"iou mean\", \"iou per class\"\r\n","\r\n","# Class for colors\r\n","class colors:\r\n","    RED       = '\\033[31;1m'\r\n","    GREEN     = '\\033[32;1m'\r\n","    YELLOW    = '\\033[33;1m'\r\n","    BLUE      = '\\033[34;1m'\r\n","    MAGENTA   = '\\033[35;1m'\r\n","    CYAN      = '\\033[36;1m'\r\n","    BOLD      = '\\033[1m'\r\n","    UNDERLINE = '\\033[4m'\r\n","    ENDC      = '\\033[0m'\r\n","\r\n","# Colored value output if colorized flag is activated.\r\n","def getColorEntry(val):\r\n","    if not isinstance(val, float):\r\n","        return colors.ENDC\r\n","    if (val < .20):\r\n","        return colors.RED\r\n","    elif (val < .40):\r\n","        return colors.YELLOW\r\n","    elif (val < .60):\r\n","        return colors.BLUE\r\n","    elif (val < .80):\r\n","        return colors.CYAN\r\n","    else:\r\n","        return colors.GREEN"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4t1QcUG_1FBi"},"source":["## Transform"]},{"cell_type":"code","metadata":{"id":"ke6O9-DT1FZq","executionInfo":{"status":"ok","timestamp":1614610935572,"user_tz":-540,"elapsed":54512,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["import numpy as np\r\n","import torch\r\n","\r\n","from PIL import Image\r\n","\r\n","def colormap_cityscapes(n):\r\n","    cmap=np.zeros([n, 3]).astype(np.uint8)\r\n","    cmap[0,:] = np.array([128, 64,128])\r\n","    cmap[1,:] = np.array([244, 35,232])\r\n","    cmap[2,:] = np.array([ 70, 70, 70])\r\n","    cmap[3,:] = np.array([ 102,102,156])\r\n","    cmap[4,:] = np.array([ 190,153,153])\r\n","    cmap[5,:] = np.array([ 153,153,153])\r\n","\r\n","    cmap[6,:] = np.array([ 250,170, 30])\r\n","    cmap[7,:] = np.array([ 220,220,  0])\r\n","    cmap[8,:] = np.array([ 107,142, 35])\r\n","    cmap[9,:] = np.array([ 152,251,152])\r\n","    cmap[10,:] = np.array([ 70,130,180])\r\n","\r\n","    cmap[11,:] = np.array([ 220, 20, 60])\r\n","    cmap[12,:] = np.array([ 255,  0,  0])\r\n","    cmap[13,:] = np.array([ 0,  0,142])\r\n","    cmap[14,:] = np.array([  0,  0, 70])\r\n","    cmap[15,:] = np.array([  0, 60,100])\r\n","\r\n","    cmap[16,:] = np.array([  0, 80,100])\r\n","    cmap[17,:] = np.array([  0,  0,230])\r\n","    cmap[18,:] = np.array([ 119, 11, 32])\r\n","    cmap[19,:] = np.array([ 0,  0,  0])\r\n","    \r\n","    return cmap\r\n","\r\n","\r\n","def colormap(n):\r\n","    cmap=np.zeros([n, 3]).astype(np.uint8)\r\n","\r\n","    for i in np.arange(n):\r\n","        r, g, b = np.zeros(3)\r\n","\r\n","        for j in np.arange(8):\r\n","            r = r + (1<<(7-j))*((i&(1<<(3*j))) >> (3*j))\r\n","            g = g + (1<<(7-j))*((i&(1<<(3*j+1))) >> (3*j+1))\r\n","            b = b + (1<<(7-j))*((i&(1<<(3*j+2))) >> (3*j+2))\r\n","\r\n","        cmap[i,:] = np.array([r, g, b])\r\n","\r\n","    return cmap\r\n","\r\n","class Relabel:\r\n","\r\n","    def __init__(self, olabel, nlabel):\r\n","        self.olabel = olabel\r\n","        self.nlabel = nlabel\r\n","\r\n","    def __call__(self, tensor):\r\n","        assert (isinstance(tensor, torch.LongTensor) or isinstance(tensor, torch.ByteTensor)) , 'tensor needs to be LongTensor'\r\n","        tensor[tensor == self.olabel] = self.nlabel\r\n","        return tensor\r\n","\r\n","\r\n","class ToLabel:\r\n","\r\n","    def __call__(self, image):\r\n","        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\r\n","\r\n","\r\n","class Colorize:\r\n","\r\n","    def __init__(self, n=22):\r\n","        #self.cmap = colormap(256)\r\n","        self.cmap = colormap_cityscapes(256)\r\n","        self.cmap[n] = self.cmap[-1]\r\n","        self.cmap = torch.from_numpy(self.cmap[:n])\r\n","\r\n","    def __call__(self, gray_image):\r\n","        size = gray_image.size()\r\n","        #print(size)\r\n","        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\r\n","        #color_image = torch.ByteTensor(3, size[0], size[1]).fill_(0)\r\n","\r\n","        #for label in range(1, len(self.cmap)):\r\n","        for label in range(0, len(self.cmap)):\r\n","            mask = gray_image[0] == label\r\n","            #mask = gray_image == label\r\n","\r\n","            color_image[0][mask] = self.cmap[label][0]\r\n","            color_image[1][mask] = self.cmap[label][1]\r\n","            color_image[2][mask] = self.cmap[label][2]\r\n","\r\n","        return color_image"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q630UOnB2KKc"},"source":["##visual"]},{"cell_type":"code","metadata":{"id":"SuobqVef2Jpt","executionInfo":{"status":"ok","timestamp":1614610935574,"user_tz":-540,"elapsed":54505,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["import numpy as np\r\n","\r\n","from torch.autograd import Variable\r\n","\r\n","from visdom import Visdom\r\n","\r\n","class Dashboard:\r\n","\r\n","    def __init__(self, port):\r\n","        self.vis = Visdom(port=port)\r\n","\r\n","    def loss(self, losses, title):\r\n","        x = np.arange(1, len(losses)+1, 1)\r\n","\r\n","        self.vis.line(losses, x, env='loss', opts=dict(title=title))\r\n","\r\n","    def image(self, image, title):\r\n","        if image.is_cuda:\r\n","            image = image.cpu()\r\n","        if isinstance(image, Variable):\r\n","            image = image.data\r\n","        image = image.numpy()\r\n","\r\n","        self.vis.image(image, env='images', opts=dict(title=title))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VWEiMU1a0w5W"},"source":["## Main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3CwiOPP1FwS","executionInfo":{"status":"ok","timestamp":1614611270723,"user_tz":-540,"elapsed":389648,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}},"outputId":"e4348e37-ff5b-4b27-e13c-341a5a4056e2"},"source":["import os\r\n","import random\r\n","import time\r\n","import numpy as np\r\n","import torch\r\n","import math\r\n","import warnings\r\n","\r\n","from PIL import Image, ImageOps\r\n","from argparse import ArgumentParser\r\n","\r\n","from torch.optim import SGD, Adam, lr_scheduler\r\n","from torch.autograd import Variable\r\n","from torch.utils.data import DataLoader\r\n","from torchvision.transforms import Compose, CenterCrop, Normalize, Resize, Pad\r\n","from torchvision.transforms import ToTensor, ToPILImage\r\n","\r\n","import importlib\r\n","\r\n","from shutil import copyfile\r\n","\r\n","\r\n","warnings.filterwarnings(action='ignore')\r\n","\r\n","NUM_CHANNELS = 3\r\n","NUM_CLASSES = 20 #pascal=22, cityscapes=20\r\n","\r\n","color_transform = Colorize(NUM_CLASSES)\r\n","image_transform = ToPILImage()\r\n","\r\n","#Augmentations - different function implemented to perform random augments on both image and target\r\n","class MyCoTransform(object):\r\n","    def __init__(self, enc, augment=True, height=512):\r\n","        self.enc=enc\r\n","        self.augment = augment\r\n","        self.height = height\r\n","        pass\r\n","    def __call__(self, input, target):\r\n","        # do something to both images\r\n","        input =  Resize(self.height, Image.BILINEAR)(input)\r\n","        target = Resize(self.height, Image.NEAREST)(target)\r\n","\r\n","        if(self.augment):\r\n","            # Random hflip\r\n","            hflip = random.random()\r\n","            if (hflip < 0.5):\r\n","                input = input.transpose(Image.FLIP_LEFT_RIGHT)\r\n","                target = target.transpose(Image.FLIP_LEFT_RIGHT)\r\n","            \r\n","            #Random translation 0-2 pixels (fill rest with padding\r\n","            transX = random.randint(-2, 2) \r\n","            transY = random.randint(-2, 2)\r\n","\r\n","            input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)\r\n","            target = ImageOps.expand(target, border=(transX,transY,0,0), fill=255) #pad label filling with 255\r\n","            input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))\r\n","            target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))   \r\n","\r\n","        input = ToTensor()(input)\r\n","        if (self.enc):\r\n","            target = Resize(int(self.height/8), Image.NEAREST)(target)\r\n","        target = ToLabel()(target)\r\n","        target = Relabel(255, 19)(target)\r\n","\r\n","        return input, target\r\n","\r\n","\r\n","class CrossEntropyLoss2d(torch.nn.Module):\r\n","\r\n","    def __init__(self, weight=None):\r\n","        super().__init__()\r\n","\r\n","        self.loss = torch.nn.NLLLoss2d(weight)\r\n","\r\n","    def forward(self, outputs, targets):\r\n","        return self.loss(torch.nn.functional.log_softmax(outputs, dim=1), targets)\r\n","\r\n","\r\n","def train(args, model, enc=False):\r\n","    best_acc = 0\r\n","\r\n","    #TODO: calculate weights by processing dataset histogram (now its being set by hand from the torch values)\r\n","    #create a loder to run all images and calculate histogram of labels, then create weight array using class balancing\r\n","\r\n","    weight = torch.ones(NUM_CLASSES)\r\n","    if (enc):\r\n","        weight[0] = 2.3653597831726\t\r\n","        weight[1] = 4.4237880706787\t\r\n","        weight[2] = 2.9691488742828\t\r\n","        weight[3] = 5.3442072868347\t\r\n","        weight[4] = 5.2983593940735\t\r\n","        weight[5] = 5.2275490760803\t\r\n","        weight[6] = 5.4394111633301\t\r\n","        weight[7] = 5.3659925460815\t\r\n","        weight[8] = 3.4170460700989\t\r\n","        weight[9] = 5.2414722442627\t\r\n","        weight[10] = 4.7376127243042\t\r\n","        weight[11] = 5.2286224365234\t\r\n","        weight[12] = 5.455126285553\t\r\n","        weight[13] = 4.3019247055054\t\r\n","        weight[14] = 5.4264230728149\t\r\n","        weight[15] = 5.4331531524658\t\r\n","        weight[16] = 5.433765411377\t\r\n","        weight[17] = 5.4631009101868\t\r\n","        weight[18] = 5.3947434425354\r\n","    else:\r\n","        weight[0] = 2.8149201869965\t\r\n","        weight[1] = 6.9850029945374\t\r\n","        weight[2] = 3.7890393733978\t\r\n","        weight[3] = 9.9428062438965\t\r\n","        weight[4] = 9.7702074050903\t\r\n","        weight[5] = 9.5110931396484\t\r\n","        weight[6] = 10.311357498169\t\r\n","        weight[7] = 10.026463508606\t\r\n","        weight[8] = 4.6323022842407\t\r\n","        weight[9] = 9.5608062744141\t\r\n","        weight[10] = 7.8698215484619\t\r\n","        weight[11] = 9.5168733596802\t\r\n","        weight[12] = 10.373730659485\t\r\n","        weight[13] = 6.6616044044495\t\r\n","        weight[14] = 10.260489463806\t\r\n","        weight[15] = 10.287888526917\t\r\n","        weight[16] = 10.289801597595\t\r\n","        weight[17] = 10.405355453491\t\r\n","        weight[18] = 10.138095855713\t\r\n","\r\n","    weight[19] = 0\r\n","\r\n","    # assert os.path.exists(args.datadir), \"Error: datadir (dataset directory) could not be loaded\"\r\n","\r\n","    co_transform = MyCoTransform(enc, augment=True, height=args.height)#1024)\r\n","    co_transform_val = MyCoTransform(enc, augment=False, height=args.height)#1024)\r\n","    dataset_train = cityscapes(args.datadir, co_transform, 'train')\r\n","    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\r\n","\r\n","    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\r\n","    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\r\n","\r\n","    if args.cuda:\r\n","        weight = weight.cuda()\r\n","    criterion = CrossEntropyLoss2d(weight)\r\n","    print(type(criterion))\r\n","\r\n","    savedir = f'{args.savedir}'\r\n","    \r\n","    if (enc):\r\n","        automated_log_path = savedir + \"/automated_log_encoder.txt\"\r\n","        modeltxtpath = savedir + \"/model_encoder.txt\"\r\n","    else:\r\n","        automated_log_path = savedir + \"/automated_log.txt\"\r\n","        modeltxtpath = savedir + \"/model.txt\"    \r\n","    \r\n","    if (not os.path.exists(automated_log_path)):    #dont add first line if it exists \r\n","        with open(automated_log_path, \"a\") as myfile:\r\n","            myfile.write(\"Epoch\\t\\tTrain-loss\\t\\tTest-loss\\t\\tTrain-IoU\\t\\tTest-IoU\\t\\tlearningRate\")\r\n","\r\n","    with open(modeltxtpath, \"w\") as myfile:\r\n","        myfile.write(str(model))\r\n","\r\n","\r\n","    #TODO: reduce memory in first gpu: https://discuss.pytorch.org/t/multi-gpu-training-memory-usage-in-balance/4163/4        #https://github.com/pytorch/pytorch/issues/1893\r\n","\r\n","    #optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=2e-4)     ## scheduler 1\r\n","    optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4)      ## scheduler 2\r\n","\r\n","    start_epoch = 1\r\n","    if args.resume:\r\n","        #Must load weights, optimizer, epoch and best value. \r\n","        if enc:\r\n","            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\r\n","        else:\r\n","            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\r\n","\r\n","        assert os.path.exists(filenameCheckpoint), \"Error: resume option was used but checkpoint was not found in folder\"\r\n","        checkpoint = torch.load(filenameCheckpoint)\r\n","        start_epoch = checkpoint['epoch']\r\n","        model.load_state_dict(checkpoint['state_dict'])\r\n","        optimizer.load_state_dict(checkpoint['optimizer'])\r\n","        best_acc = checkpoint['best_acc']\r\n","        print(\"=> Loaded checkpoint at epoch {})\".format(checkpoint['epoch']))\r\n","\r\n","    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5) # set up scheduler     ## scheduler 1\r\n","    lambda1 = lambda epoch: pow((1-((epoch-1)/args.num_epochs)),0.9)  ## scheduler 2\r\n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)                             ## scheduler 2\r\n","\r\n","    if args.visualize and args.steps_plot > 0:\r\n","        board = Dashboard(args.port)\r\n","\r\n","    for epoch in range(start_epoch, args.num_epochs+1):\r\n","        print(\"----- TRAINING - EPOCH\", epoch, \"-----\")\r\n","\r\n","        scheduler.step(epoch)    ## scheduler 2\r\n","\r\n","        epoch_loss = []\r\n","        time_train = []\r\n","     \r\n","        doIouTrain = args.iouTrain   \r\n","        doIouVal =  args.iouVal      \r\n","\r\n","        if (doIouTrain):\r\n","            iouEvalTrain = iouEval(NUM_CLASSES)\r\n","\r\n","        usedLr = 0\r\n","        for param_group in optimizer.param_groups:\r\n","            print(\"LEARNING RATE: \", param_group['lr'])\r\n","            usedLr = float(param_group['lr'])\r\n","\r\n","        model.train()\r\n","        for step, (images, labels) in enumerate(loader):\r\n","\r\n","            start_time = time.time()\r\n","            #print (labels.size())\r\n","            #print (np.unique(labels.numpy()))\r\n","            #print(\"labels: \", np.unique(labels[0].numpy()))\r\n","            #labels = torch.ones(4, 1, 512, 1024).long()\r\n","            if args.cuda:\r\n","                images = images.cuda()\r\n","                labels = labels.cuda()\r\n","\r\n","            inputs = Variable(images)\r\n","            targets = Variable(labels)\r\n","            outputs = model(inputs, only_encode=enc)\r\n","\r\n","            #print(\"targets\", np.unique(targets[:, 0].cpu().data.numpy()))\r\n","\r\n","            optimizer.zero_grad()\r\n","            loss = criterion(outputs, targets[:, 0])\r\n","            loss.backward()\r\n","            optimizer.step()\r\n","\r\n","            epoch_loss.append(loss.item())\r\n","            time_train.append(time.time() - start_time)\r\n","\r\n","            if (doIouTrain):\r\n","                #start_time_iou = time.time()\r\n","                iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\r\n","                #print (\"Time to add confusion matrix: \", time.time() - start_time_iou)      \r\n","\r\n","            #print(outputs.size())\r\n","            if args.visualize and args.steps_plot > 0 and step % args.steps_plot == 0:\r\n","                start_time_plot = time.time()\r\n","                image = inputs[0].cpu().data\r\n","                #image[0] = image[0] * .229 + .485\r\n","                #image[1] = image[1] * .224 + .456\r\n","                #image[2] = image[2] * .225 + .406\r\n","                #print(\"output\", np.unique(outputs[0].cpu().max(0)[1].data.numpy()))\r\n","                board.image(image, f'input (epoch: {epoch}, step: {step})')\r\n","                if isinstance(outputs, list):   #merge gpu tensors\r\n","                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\r\n","                    f'output (epoch: {epoch}, step: {step})')\r\n","                else:\r\n","                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\r\n","                    f'output (epoch: {epoch}, step: {step})')\r\n","                board.image(color_transform(targets[0].cpu().data),\r\n","                    f'target (epoch: {epoch}, step: {step})')\r\n","                print (\"Time to paint images: \", time.time() - start_time_plot)\r\n","            if args.steps_loss > 0 and step % args.steps_loss == 0:\r\n","                average = sum(epoch_loss) / len(epoch_loss)\r\n","                print(f'loss: {average:0.4} (epoch: {epoch}, step: {step})', \r\n","                        \"// Avg time/img: %.4f s\" % (sum(time_train) / len(time_train) / args.batch_size))\r\n","\r\n","            \r\n","        average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\r\n","        \r\n","        iouTrain = 0\r\n","        if (doIouTrain):\r\n","            iouTrain, iou_classes = iouEvalTrain.getIoU()\r\n","            iouStr = getColorEntry(iouTrain)+'{:0.2f}'.format(iouTrain*100) + '\\033[0m'\r\n","            print (\"EPOCH IoU on TRAIN set: \", iouStr, \"%\")  \r\n","\r\n","        #Validate on 500 val images after each epoch of training\r\n","        print(\"----- VALIDATING - EPOCH\", epoch, \"-----\")\r\n","        model.eval()\r\n","        epoch_loss_val = []\r\n","        time_val = []\r\n","        \r\n","        if (doIouVal):\r\n","            iouEvalVal = iouEval(NUM_CLASSES)\r\n","\r\n","        for step, (images, labels) in enumerate(loader_val):\r\n","            start_time = time.time()\r\n","            if args.cuda:\r\n","                images = images.cuda()\r\n","                labels = labels.cuda()\r\n","\r\n","            inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\r\n","            targets = Variable(labels, volatile=True)\r\n","            outputs = model(inputs, only_encode=enc) \r\n","\r\n","            loss = criterion(outputs, targets[:, 0])\r\n","            epoch_loss_val.append(loss.item())\r\n","            time_val.append(time.time() - start_time)\r\n","\r\n","\r\n","            #Add batch to calculate TP, FP and FN for iou estimation\r\n","            if (doIouVal):\r\n","                #start_time_iou = time.time()\r\n","                iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\r\n","                #print (\"Time to add confusion matrix: \", time.time() - start_time_iou)\r\n","\r\n","            if args.visualize and args.steps_plot > 0 and step % args.steps_plot == 0:\r\n","                start_time_plot = time.time()\r\n","                image = inputs[0].cpu().data\r\n","                board.image(image, f'VAL input (epoch: {epoch}, step: {step})')\r\n","                if isinstance(outputs, list):   #merge gpu tensors\r\n","                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\r\n","                    f'VAL output (epoch: {epoch}, step: {step})')\r\n","                else:\r\n","                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\r\n","                    f'VAL output (epoch: {epoch}, step: {step})')\r\n","                board.image(color_transform(targets[0].cpu().data),\r\n","                    f'VAL target (epoch: {epoch}, step: {step})')\r\n","                print (\"Time to paint images: \", time.time() - start_time_plot)\r\n","            if args.steps_loss > 0 and step % args.steps_loss == 0:\r\n","                average = sum(epoch_loss_val) / len(epoch_loss_val)\r\n","                print(f'VAL loss: {average:0.4} (epoch: {epoch}, step: {step})', \r\n","                        \"// Avg time/img: %.4f s\" % (sum(time_val) / len(time_val) / args.batch_size))\r\n","                       \r\n","\r\n","        average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)\r\n","        #scheduler.step(average_epoch_loss_val, epoch)  ## scheduler 1   # update lr if needed\r\n","\r\n","        iouVal = 0\r\n","        if (doIouVal):\r\n","            iouVal, iou_classes = iouEvalVal.getIoU()\r\n","            iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\r\n","            print (\"EPOCH IoU on VAL set: \", iouStr, \"%\") \r\n","           \r\n","\r\n","        # remember best valIoU and save checkpoint\r\n","        if iouVal == 0:\r\n","            current_acc = -average_epoch_loss_val\r\n","        else:\r\n","            current_acc = iouVal \r\n","        is_best = current_acc > best_acc\r\n","        best_acc = max(current_acc, best_acc)\r\n","        if enc:\r\n","            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\r\n","            filenameBest = savedir + '/model_best_enc.pth.tar'    \r\n","        else:\r\n","            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\r\n","            filenameBest = savedir + '/model_best.pth.tar'\r\n","        save_checkpoint({\r\n","            'epoch': epoch + 1,\r\n","            'arch': str(model),\r\n","            'state_dict': model.state_dict(),\r\n","            'best_acc': best_acc,\r\n","            'optimizer' : optimizer.state_dict(),\r\n","        }, is_best, filenameCheckpoint, filenameBest)\r\n","\r\n","        #SAVE MODEL AFTER EPOCH\r\n","        if (enc):\r\n","            filename = f'{savedir}/model_encoder-{epoch:03}.pth'\r\n","            filenamebest = f'{savedir}/model_encoder_best.pth'\r\n","        else:\r\n","            filename = f'{savedir}/model-{epoch:03}.pth'\r\n","            filenamebest = f'{savedir}/model_best.pth'\r\n","        if args.epochs_save > 0 and step > 0 and step % args.epochs_save == 0:\r\n","            torch.save(model.state_dict(), filename)\r\n","            print(f'save: {filename} (epoch: {epoch})')\r\n","        if (is_best):\r\n","            torch.save(model.state_dict(), filenamebest)\r\n","            print(f'save: {filenamebest} (epoch: {epoch})')\r\n","            if (not enc):\r\n","                with open(savedir + \"/best.txt\", \"w\") as myfile:\r\n","                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))   \r\n","            else:\r\n","                with open(savedir + \"/best_encoder.txt\", \"w\") as myfile:\r\n","                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))           \r\n","\r\n","        #SAVE TO FILE A ROW WITH THE EPOCH RESULT (train loss, val loss, train IoU, val IoU)\r\n","        #Epoch\t\tTrain-loss\t\tTest-loss\tTrain-IoU\tTest-IoU\t\tlearningRate\r\n","        with open(automated_log_path, \"a\") as myfile:\r\n","            myfile.write(\"\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.8f\" % (epoch, average_epoch_loss_train, average_epoch_loss_val, iouTrain, iouVal, usedLr ))\r\n","    \r\n","    return(model)   #return model (convenience for encoder-decoder training)\r\n","\r\n","def save_checkpoint(state, is_best, filenameCheckpoint, filenameBest):\r\n","    torch.save(state, filenameCheckpoint)\r\n","    if is_best:\r\n","        print (\"Saving model as best\")\r\n","        torch.save(state, filenameBest)\r\n","\r\n","\r\n","def main(args):\r\n","    savedir = f'/save/{args.savedir}'\r\n","\r\n","    if not os.path.exists(savedir):\r\n","        os.makedirs(savedir)\r\n","\r\n","    with open(savedir + '/opts.txt', \"w\") as myfile:\r\n","        myfile.write(str(args))\r\n","\r\n","    #Load Model\r\n","    # assert os.path.exists(args.model + \".py\"), \"Error: model definition not found\"\r\n","    # model_file = importlib.import_module(args.model)\r\n","    model = Net(NUM_CLASSES)\r\n","    # copyfile(args.model + \".py\", savedir + '/' + args.model + \".py\")\r\n","    \r\n","    if args.cuda:\r\n","        model = torch.nn.DataParallel(model).cuda()\r\n","    \r\n","    if args.state:\r\n","        #if args.state is provided then load this state for training\r\n","        #Note: this only loads initialized weights. If you want to resume a training use \"--resume\" option!!\r\n","        \"\"\"\r\n","        try:\r\n","            model.load_state_dict(torch.load(args.state))\r\n","        except AssertionError:\r\n","            model.load_state_dict(torch.load(args.state,\r\n","                map_location=lambda storage, loc: storage))\r\n","        #When model is saved as DataParallel it adds a model. to each key. To remove:\r\n","        #state_dict = {k.partition('model.')[2]: v for k,v in state_dict}\r\n","        #https://discuss.pytorch.org/t/prefix-parameter-names-in-saved-model-if-trained-by-multi-gpu/494\r\n","        \"\"\"\r\n","        def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict keys are there\r\n","            own_state = model.state_dict()\r\n","            for name, param in state_dict.items():\r\n","                if name not in own_state:\r\n","                     continue\r\n","                own_state[name].copy_(param)\r\n","            return model\r\n","\r\n","        #print(torch.load(args.state))\r\n","        model = load_my_state_dict(model, torch.load(args.state))\r\n","\r\n","    \"\"\"\r\n","    def weights_init(m):\r\n","        classname = m.__class__.__name__\r\n","        if classname.find('Conv') != -1:\r\n","            #m.weight.data.normal_(0.0, 0.02)\r\n","            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n","            m.weight.data.normal_(0, math.sqrt(2. / n))\r\n","        elif classname.find('BatchNorm') != -1:\r\n","            #m.weight.data.normal_(1.0, 0.02)\r\n","            m.weight.data.fill_(1)\r\n","            m.bias.data.fill_(0)\r\n","    #TO ACCESS MODEL IN DataParallel: next(model.children())\r\n","    #next(model.children()).decoder.apply(weights_init)\r\n","    #Reinitialize weights for decoder\r\n","    \r\n","    next(model.children()).decoder.layers.apply(weights_init)\r\n","    next(model.children()).decoder.output_conv.apply(weights_init)\r\n","    #print(model.state_dict())\r\n","    f = open('weights5.txt', 'w')\r\n","    f.write(str(model.state_dict()))\r\n","    f.close()\r\n","    \"\"\"\r\n","\r\n","    # train(args, model)\r\n","    if (not args.decoder):\r\n","        print(\"========== ENCODER TRAINING ===========\")\r\n","        model = train(args, model, True) #Train encoder\r\n","    #CAREFUL: for some reason, after training encoder alone, the decoder gets weights=0. \r\n","    #We must reinit decoder weights or reload network passing only encoder in order to train decoder\r\n","    print(\"========== DECODER TRAINING ===========\")\r\n","    if (not args.state):\r\n","        if args.pretrainedEncoder:\r\n","            print(\"Loading encoder pretrained in imagenet\")\r\n","            from erfnet_imagenet import ERFNet as ERFNet_imagenet\r\n","            pretrainedEnc = torch.nn.DataParallel(ERFNet_imagenet(1000))\r\n","            pretrainedEnc.load_state_dict(torch.load(args.pretrainedEncoder)['state_dict'])\r\n","            pretrainedEnc = next(pretrainedEnc.children()).features.encoder\r\n","            if (not args.cuda):\r\n","                pretrainedEnc = pretrainedEnc.cpu()     #because loaded encoder is probably saved in cuda\r\n","        else:\r\n","            pretrainedEnc = next(model.children()).encoder\r\n","        model = Net(NUM_CLASSES, encoder=pretrainedEnc)  #Add decoder to encoder\r\n","        if args.cuda:\r\n","            model = torch.nn.DataParallel(model).cuda()\r\n","        #When loading encoder reinitialize weights for decoder because they are set to 0 when training dec\r\n","    model = train(args, model, False)   #Train decoder\r\n","    print(\"========== TRAINING FINISHED ===========\")\r\n","\r\n","if __name__ == '__main__':\r\n","    parser = ArgumentParser()\r\n","    parser.add_argument('--cuda', action='store_true', default=True)  #NOTE: cpu-only has not been tested so you might have to change code if you deactivate this flag\r\n","    parser.add_argument('--model', default=\"erfnet\")\r\n","    parser.add_argument('--state')\r\n","\r\n","    parser.add_argument('--port', type=int, default=8097)\r\n","    parser.add_argument('--datadir', default='/content/gdrive/MyDrive/cityscapes')\r\n","    parser.add_argument('--height', type=int, default=512)\r\n","    parser.add_argument('--num-epochs', type=int, default=5)\r\n","    parser.add_argument('--num-workers', type=int, default=4)\r\n","    parser.add_argument('--batch-size', type=int, default=3)\r\n","    parser.add_argument('--steps-loss', type=int, default=50)\r\n","    parser.add_argument('--steps-plot', type=int, default=50)\r\n","    parser.add_argument('--epochs-save', type=int, default=0)    #You can use this value to save model every X epochs\r\n","    parser.add_argument('--savedir', type=str, default='/content/gdrive/MyDrive/save')\r\n","    parser.add_argument('--decoder', action='store_true')\r\n","    parser.add_argument('--pretrainedEncoder') #, default=\"../trained_models/erfnet_encoder_pretrained.pth.tar\")\r\n","    parser.add_argument('--visualize', action='store_true', default=True)\r\n","\r\n","    parser.add_argument('--iouTrain', action='store_true', default=False) #recommended: False (takes more time to train otherwise)\r\n","    parser.add_argument('--iouVal', action='store_true', default=True)  \r\n","    parser.add_argument('--resume', action='store_true')    #Use this flag to load last checkpoint for training  \r\n","\r\n","    main(parser.parse_args(''))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["========== ENCODER TRAINING ===========\n","/content/gdrive/MyDrive/cityscapes/leftImg8bit/train\n","/content/gdrive/MyDrive/cityscapes/leftImg8bit/val\n","<class '__main__.CrossEntropyLoss2d'>\n"],"name":"stdout"},{"output_type":"stream","text":["Setting up a new session...\n"],"name":"stderr"},{"output_type":"stream","text":["----- TRAINING - EPOCH 1 -----\n","LEARNING RATE:  0.0005\n","Time to paint images:  0.274552583694458\n","loss: 3.999 (epoch: 1, step: 0) // Avg time/img: 0.1266 s\n","----- VALIDATING - EPOCH 1 -----\n","Time to paint images:  0.3519105911254883\n","VAL loss: 4.053 (epoch: 1, step: 0) // Avg time/img: 0.0154 s\n","EPOCH IoU on VAL set:  \u001b[0m4.38\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_encoder_best.pth (epoch: 1)\n","----- TRAINING - EPOCH 2 -----\n","LEARNING RATE:  0.00040902607302542923\n","Time to paint images:  0.8835418224334717\n","loss: 1.088 (epoch: 2, step: 0) // Avg time/img: 0.1311 s\n","----- VALIDATING - EPOCH 2 -----\n","Time to paint images:  0.9484984874725342\n","VAL loss: 2.055 (epoch: 2, step: 0) // Avg time/img: 0.0211 s\n","EPOCH IoU on VAL set:  \u001b[0m12.29\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_encoder_best.pth (epoch: 2)\n","----- TRAINING - EPOCH 3 -----\n","LEARNING RATE:  0.00031572293374467766\n","Time to paint images:  0.9517374038696289\n","loss: 0.9159 (epoch: 3, step: 0) // Avg time/img: 0.1213 s\n","----- VALIDATING - EPOCH 3 -----\n","Time to paint images:  0.9602484703063965\n","VAL loss: 1.718 (epoch: 3, step: 0) // Avg time/img: 0.0265 s\n","EPOCH IoU on VAL set:  \u001b[0m12.45\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_encoder_best.pth (epoch: 3)\n","----- TRAINING - EPOCH 4 -----\n","LEARNING RATE:  0.00021919164527704348\n","Time to paint images:  1.0150909423828125\n","loss: 1.034 (epoch: 4, step: 0) // Avg time/img: 0.1236 s\n","----- VALIDATING - EPOCH 4 -----\n","Time to paint images:  1.011120080947876\n","VAL loss: 1.78 (epoch: 4, step: 0) // Avg time/img: 0.0285 s\n","EPOCH IoU on VAL set:  \u001b[0m14.86\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_encoder_best.pth (epoch: 4)\n","----- TRAINING - EPOCH 5 -----\n","LEARNING RATE:  0.00011746189430880188\n","Time to paint images:  0.9218916893005371\n","loss: 0.7567 (epoch: 5, step: 0) // Avg time/img: 0.1216 s\n","----- VALIDATING - EPOCH 5 -----\n","Time to paint images:  1.030372142791748\n","VAL loss: 2.017 (epoch: 5, step: 0) // Avg time/img: 0.0221 s\n","EPOCH IoU on VAL set:  \u001b[0m14.56\u001b[0m %\n","========== DECODER TRAINING ===========\n","/content/gdrive/MyDrive/cityscapes/leftImg8bit/train\n","/content/gdrive/MyDrive/cityscapes/leftImg8bit/val\n","<class '__main__.CrossEntropyLoss2d'>\n"],"name":"stdout"},{"output_type":"stream","text":["Setting up a new session...\n"],"name":"stderr"},{"output_type":"stream","text":["----- TRAINING - EPOCH 1 -----\n","LEARNING RATE:  0.0005\n","Time to paint images:  1.3273043632507324\n","loss: 3.022 (epoch: 1, step: 0) // Avg time/img: 0.1533 s\n","----- VALIDATING - EPOCH 1 -----\n","Time to paint images:  1.4837539196014404\n","VAL loss: 2.524 (epoch: 1, step: 0) // Avg time/img: 0.0352 s\n","EPOCH IoU on VAL set:  \u001b[0m5.41\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_best.pth (epoch: 1)\n","----- TRAINING - EPOCH 2 -----\n","LEARNING RATE:  0.00040902607302542923\n","Time to paint images:  1.4224555492401123\n","loss: 2.546 (epoch: 2, step: 0) // Avg time/img: 0.1578 s\n","----- VALIDATING - EPOCH 2 -----\n","Time to paint images:  1.2289419174194336\n","VAL loss: 2.355 (epoch: 2, step: 0) // Avg time/img: 0.0317 s\n","EPOCH IoU on VAL set:  \u001b[0m8.03\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_best.pth (epoch: 2)\n","----- TRAINING - EPOCH 3 -----\n","LEARNING RATE:  0.00031572293374467766\n","Time to paint images:  1.2714135646820068\n","loss: 2.259 (epoch: 3, step: 0) // Avg time/img: 0.1604 s\n","----- VALIDATING - EPOCH 3 -----\n","Time to paint images:  1.3990848064422607\n","VAL loss: 2.025 (epoch: 3, step: 0) // Avg time/img: 0.0308 s\n","EPOCH IoU on VAL set:  \u001b[0m11.31\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_best.pth (epoch: 3)\n","----- TRAINING - EPOCH 4 -----\n","LEARNING RATE:  0.00021919164527704348\n","Time to paint images:  1.4690980911254883\n","loss: 1.875 (epoch: 4, step: 0) // Avg time/img: 0.1484 s\n","----- VALIDATING - EPOCH 4 -----\n","Time to paint images:  1.2274973392486572\n","VAL loss: 1.918 (epoch: 4, step: 0) // Avg time/img: 0.0386 s\n","EPOCH IoU on VAL set:  \u001b[0m12.22\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_best.pth (epoch: 4)\n","----- TRAINING - EPOCH 5 -----\n","LEARNING RATE:  0.00011746189430880188\n","Time to paint images:  1.3215653896331787\n","loss: 2.017 (epoch: 5, step: 0) // Avg time/img: 0.1610 s\n","----- VALIDATING - EPOCH 5 -----\n","Time to paint images:  1.3316471576690674\n","VAL loss: 1.965 (epoch: 5, step: 0) // Avg time/img: 0.0318 s\n","EPOCH IoU on VAL set:  \u001b[0m13.06\u001b[0m %\n","Saving model as best\n","save: /content/gdrive/MyDrive/save/model_best.pth (epoch: 5)\n","========== TRAINING FINISHED ===========\n"],"name":"stdout"}]}]}