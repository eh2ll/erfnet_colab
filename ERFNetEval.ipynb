{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ERFNetEval.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"history_visible":true,"authorship_tag":"ABX9TyNY9Cahp67K4WDUpTUfsqBn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4GZDwa6M5sCQ"},"source":["# connect google"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBh__3akDSz7","executionInfo":{"status":"ok","timestamp":1614607386245,"user_tz":-540,"elapsed":988,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}},"outputId":"0539a1ac-90e5-4db1-b7e7-45d11a3b711e"},"source":["from google.colab import drive \r\n","\r\n","drive.mount('/content/gdrive/')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nEfRGAEA51P_"},"source":["# dataset"]},{"cell_type":"code","metadata":{"id":"hSFqLJQF55tt","executionInfo":{"status":"ok","timestamp":1614607386500,"user_tz":-540,"elapsed":1230,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["# Code with dataset loader for VOC12 and Cityscapes (adapted from bodokaiser/piwise code)\r\n","# Sept 2017\r\n","# Eduardo Romera\r\n","#######################\r\n","\r\n","import numpy as np\r\n","import os\r\n","\r\n","from PIL import Image\r\n","\r\n","from torch.utils.data import Dataset\r\n","\r\n","EXTENSIONS = ['.jpg', '.png']\r\n","\r\n","def load_image(file):\r\n","    return Image.open(file)\r\n","\r\n","def is_image(filename):\r\n","    return any(filename.endswith(ext) for ext in EXTENSIONS)\r\n","\r\n","def is_label(filename):\r\n","    return filename.endswith(\"_labelTrainIds.png\")\r\n","\r\n","def image_path(root, basename, extension):\r\n","    return os.path.join(root, f'{basename}{extension}')\r\n","\r\n","def image_path_city(root, name):\r\n","    return os.path.join(root, f'{name}')\r\n","\r\n","def image_basename(filename):\r\n","    return os.path.basename(os.path.splitext(filename)[0])\r\n","\r\n","class VOC12(Dataset):\r\n","\r\n","    def __init__(self, root, input_transform=None, target_transform=None):\r\n","        self.images_root = os.path.join(root, 'images')\r\n","        self.labels_root = os.path.join(root, 'labels')\r\n","\r\n","        self.filenames = [image_basename(f)\r\n","            for f in os.listdir(self.labels_root) if is_image(f)]\r\n","        self.filenames.sort()\r\n","\r\n","        self.input_transform = input_transform\r\n","        self.target_transform = target_transform\r\n","\r\n","    def __getitem__(self, index):\r\n","        filename = self.filenames[index]\r\n","\r\n","        with open(image_path(self.images_root, filename, '.jpg'), 'rb') as f:\r\n","            image = load_image(f).convert('RGB')\r\n","        with open(image_path(self.labels_root, filename, '.png'), 'rb') as f:\r\n","            label = load_image(f).convert('P')\r\n","\r\n","        if self.input_transform is not None:\r\n","            image = self.input_transform(image)\r\n","        if self.target_transform is not None:\r\n","            label = self.target_transform(label)\r\n","\r\n","        return image, label\r\n","\r\n","    def __len__(self):\r\n","        return len(self.filenames)\r\n","\r\n","\r\n","class cityscapes(Dataset):\r\n","\r\n","    def __init__(self, root, input_transform=None, target_transform=None, subset='val'):\r\n","        self.images_root = os.path.join(root, 'leftImg8bit/' + subset)\r\n","        self.labels_root = os.path.join(root, 'gtFine/' + subset)\r\n","\r\n","        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\r\n","        self.filenames.sort()\r\n","\r\n","        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\r\n","        self.filenamesGt.sort()\r\n","\r\n","        self.input_transform = input_transform\r\n","        self.target_transform = target_transform\r\n","\r\n","    def __getitem__(self, index):\r\n","        filename = self.filenames[index]\r\n","        filenameGt = self.filenamesGt[index]\r\n","\r\n","        #print(filename)\r\n","\r\n","        with open(image_path_city(self.images_root, filename), 'rb') as f:\r\n","            image = load_image(f).convert('RGB')\r\n","        with open(image_path_city(self.labels_root, filenameGt), 'rb') as f:\r\n","            label = load_image(f).convert('P')\r\n","\r\n","        if self.input_transform is not None:\r\n","            image = self.input_transform(image)\r\n","        if self.target_transform is not None:\r\n","            label = self.target_transform(label)\r\n","\r\n","        return image, label, filename, filenameGt\r\n","\r\n","    def __len__(self):\r\n","        return len(self.filenames)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeZY7vn56GOo"},"source":["# erfnet"]},{"cell_type":"code","metadata":{"id":"nydwsSco6L4a","executionInfo":{"status":"ok","timestamp":1614607386850,"user_tz":-540,"elapsed":1575,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["# ERFNET full network definition for Pytorch\r\n","# Sept 2017\r\n","# Eduardo Romera\r\n","#######################\r\n","\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.init as init\r\n","import torch.nn.functional as F\r\n","\r\n","\r\n","class DownsamplerBlock (nn.Module):\r\n","    def __init__(self, ninput, noutput):\r\n","        super().__init__()\r\n","\r\n","        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\r\n","        self.pool = nn.MaxPool2d(2, stride=2)\r\n","        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\r\n","\r\n","    def forward(self, input):\r\n","        output = torch.cat([self.conv(input), self.pool(input)], 1)\r\n","        output = self.bn(output)\r\n","        return F.relu(output)\r\n","    \r\n","\r\n","class non_bottleneck_1d (nn.Module):\r\n","    def __init__(self, chann, dropprob, dilated):        \r\n","        super().__init__()\r\n","\r\n","        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\r\n","\r\n","        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\r\n","\r\n","        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\r\n","\r\n","        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\r\n","\r\n","        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\r\n","\r\n","        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\r\n","\r\n","        self.dropout = nn.Dropout2d(dropprob)\r\n","        \r\n","\r\n","    def forward(self, input):\r\n","\r\n","        output = self.conv3x1_1(input)\r\n","        output = F.relu(output)\r\n","        output = self.conv1x3_1(output)\r\n","        output = self.bn1(output)\r\n","        output = F.relu(output)\r\n","\r\n","        output = self.conv3x1_2(output)\r\n","        output = F.relu(output)\r\n","        output = self.conv1x3_2(output)\r\n","        output = self.bn2(output)\r\n","\r\n","        if (self.dropout.p != 0):\r\n","            output = self.dropout(output)\r\n","        \r\n","        return F.relu(output+input)    #+input = identity (residual connection)\r\n","\r\n","\r\n","class Encoder(nn.Module):\r\n","    def __init__(self, num_classes):\r\n","        super().__init__()\r\n","        self.initial_block = DownsamplerBlock(3,16)\r\n","\r\n","        self.layers = nn.ModuleList()\r\n","\r\n","        self.layers.append(DownsamplerBlock(16,64))\r\n","\r\n","        for x in range(0, 5):    #5 times\r\n","           self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \r\n","\r\n","        self.layers.append(DownsamplerBlock(64,128))\r\n","\r\n","        for x in range(0, 2):    #2 times\r\n","            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\r\n","            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\r\n","            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\r\n","            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\r\n","\r\n","        #only for encoder mode:\r\n","        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\r\n","\r\n","    def forward(self, input, predict=False):\r\n","        output = self.initial_block(input)\r\n","\r\n","        for layer in self.layers:\r\n","            output = layer(output)\r\n","\r\n","        if predict:\r\n","            output = self.output_conv(output)\r\n","\r\n","        return output\r\n","\r\n","\r\n","class UpsamplerBlock (nn.Module):\r\n","    def __init__(self, ninput, noutput):\r\n","        super().__init__()\r\n","        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\r\n","        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\r\n","\r\n","    def forward(self, input):\r\n","        output = self.conv(input)\r\n","        output = self.bn(output)\r\n","        return F.relu(output)\r\n","\r\n","class Decoder (nn.Module):\r\n","    def __init__(self, num_classes):\r\n","        super().__init__()\r\n","\r\n","        self.layers = nn.ModuleList()\r\n","\r\n","        self.layers.append(UpsamplerBlock(128,64))\r\n","        self.layers.append(non_bottleneck_1d(64, 0, 1))\r\n","        self.layers.append(non_bottleneck_1d(64, 0, 1))\r\n","\r\n","        self.layers.append(UpsamplerBlock(64,16))\r\n","        self.layers.append(non_bottleneck_1d(16, 0, 1))\r\n","        self.layers.append(non_bottleneck_1d(16, 0, 1))\r\n","\r\n","        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\r\n","\r\n","    def forward(self, input):\r\n","        output = input\r\n","\r\n","        for layer in self.layers:\r\n","            output = layer(output)\r\n","\r\n","        output = self.output_conv(output)\r\n","\r\n","        return output\r\n","\r\n","\r\n","class ERFNet(nn.Module):\r\n","    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\r\n","        super().__init__()\r\n","\r\n","        if (encoder == None):\r\n","            self.encoder = Encoder(num_classes)\r\n","        else:\r\n","            self.encoder = encoder\r\n","        self.decoder = Decoder(num_classes)\r\n","\r\n","    def forward(self, input, only_encode=False):\r\n","        if only_encode:\r\n","            return self.encoder.forward(input, predict=True)\r\n","        else:\r\n","            output = self.encoder(input)    #predict=False by default\r\n","            return self.decoder.forward(output)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahMrTxUB6Qn7"},"source":["#iou eval"]},{"cell_type":"code","metadata":{"id":"06MZNTLV6SUr","executionInfo":{"status":"ok","timestamp":1614607386868,"user_tz":-540,"elapsed":1589,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["# Code for evaluating IoU \r\n","# Nov 2017\r\n","# Eduardo Romera\r\n","#######################\r\n","\r\n","import torch\r\n","\r\n","class iouEval:\r\n","\r\n","    def __init__(self, nClasses, ignoreIndex=19):\r\n","        self.nClasses = nClasses\r\n","        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\r\n","        self.reset()\r\n","\r\n","    def reset (self):\r\n","        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\r\n","        self.tp = torch.zeros(classes).double()\r\n","        self.fp = torch.zeros(classes).double()\r\n","        self.fn = torch.zeros(classes).double()        \r\n","\r\n","    def addBatch(self, x, y):   #x=preds, y=targets\r\n","        #sizes should be \"batch_size x nClasses x H x W\"\r\n","        \r\n","        #print (\"X is cuda: \", x.is_cuda)\r\n","        #print (\"Y is cuda: \", y.is_cuda)\r\n","\r\n","        if (x.is_cuda or y.is_cuda):\r\n","            x = x.cuda()\r\n","            y = y.cuda()\r\n","\r\n","        #if size is \"batch_size x 1 x H x W\" scatter to onehot\r\n","        if (x.size(1) == 1):\r\n","            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \r\n","            if x.is_cuda:\r\n","                x_onehot = x_onehot.cuda()\r\n","            x_onehot.scatter_(1, x, 1).float()\r\n","        else:\r\n","            x_onehot = x.float()\r\n","\r\n","        if (y.size(1) == 1):\r\n","            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\r\n","            if y.is_cuda:\r\n","                y_onehot = y_onehot.cuda()\r\n","            y_onehot.scatter_(1, y, 1).float()\r\n","        else:\r\n","            y_onehot = y.float()\r\n","\r\n","        if (self.ignoreIndex != -1): \r\n","            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\r\n","            x_onehot = x_onehot[:, :self.ignoreIndex]\r\n","            y_onehot = y_onehot[:, :self.ignoreIndex]\r\n","        else:\r\n","            ignores=0\r\n","\r\n","        #print(type(x_onehot))\r\n","        #print(type(y_onehot))\r\n","        #print(x_onehot.size())\r\n","        #print(y_onehot.size())\r\n","\r\n","        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\r\n","        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\r\n","        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\r\n","        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\r\n","        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\r\n","        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \r\n","\r\n","        self.tp += tp.double().cpu()\r\n","        self.fp += fp.double().cpu()\r\n","        self.fn += fn.double().cpu()\r\n","\r\n","    def getIoU(self):\r\n","        num = self.tp\r\n","        den = self.tp + self.fp + self.fn + 1e-15\r\n","        iou = num / den\r\n","        return torch.mean(iou), iou     #returns \"iou mean\", \"iou per class\"\r\n","\r\n","# Class for colors\r\n","class colors:\r\n","    RED       = '\\033[31;1m'\r\n","    GREEN     = '\\033[32;1m'\r\n","    YELLOW    = '\\033[33;1m'\r\n","    BLUE      = '\\033[34;1m'\r\n","    MAGENTA   = '\\033[35;1m'\r\n","    CYAN      = '\\033[36;1m'\r\n","    BOLD      = '\\033[1m'\r\n","    UNDERLINE = '\\033[4m'\r\n","    ENDC      = '\\033[0m'\r\n","\r\n","# Colored value output if colorized flag is activated.\r\n","def getColorEntry(val):\r\n","    if not isinstance(val, float):\r\n","        return colors.ENDC\r\n","    if (val < .20):\r\n","        return colors.RED\r\n","    elif (val < .40):\r\n","        return colors.YELLOW\r\n","    elif (val < .60):\r\n","        return colors.BLUE\r\n","    elif (val < .80):\r\n","        return colors.CYAN\r\n","    else:\r\n","        return colors.GREEN"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQ0PAlGR6Tl4"},"source":["#transform"]},{"cell_type":"code","metadata":{"id":"21pctpv46VE9","executionInfo":{"status":"ok","timestamp":1614607386870,"user_tz":-540,"elapsed":1587,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}}},"source":["# Code with transformations for Cityscapes (adapted from bodokaiser/piwise code)\r\n","# Sept 2017\r\n","# Eduardo Romera\r\n","#######################\r\n","\r\n","import numpy as np\r\n","import torch\r\n","\r\n","from PIL import Image\r\n","\r\n","def colormap_cityscapes(n):\r\n","    cmap=np.zeros([n, 3]).astype(np.uint8)\r\n","    cmap[0,:] = np.array([128, 64,128])\r\n","    cmap[1,:] = np.array([244, 35,232])\r\n","    cmap[2,:] = np.array([ 70, 70, 70])\r\n","    cmap[3,:] = np.array([ 102,102,156])\r\n","    cmap[4,:] = np.array([ 190,153,153])\r\n","    cmap[5,:] = np.array([ 153,153,153])\r\n","\r\n","    cmap[6,:] = np.array([ 250,170, 30])\r\n","    cmap[7,:] = np.array([ 220,220,  0])\r\n","    cmap[8,:] = np.array([ 107,142, 35])\r\n","    cmap[9,:] = np.array([ 152,251,152])\r\n","    cmap[10,:] = np.array([ 70,130,180])\r\n","\r\n","    cmap[11,:] = np.array([ 220, 20, 60])\r\n","    cmap[12,:] = np.array([ 255,  0,  0])\r\n","    cmap[13,:] = np.array([ 0,  0,142])\r\n","    cmap[14,:] = np.array([  0,  0, 70])\r\n","    cmap[15,:] = np.array([  0, 60,100])\r\n","\r\n","    cmap[16,:] = np.array([  0, 80,100])\r\n","    cmap[17,:] = np.array([  0,  0,230])\r\n","    cmap[18,:] = np.array([ 119, 11, 32])\r\n","    cmap[19,:] = np.array([ 0,  0,  0])\r\n","    \r\n","    return cmap\r\n","\r\n","\r\n","def colormap(n):\r\n","    cmap=np.zeros([n, 3]).astype(np.uint8)\r\n","\r\n","    for i in np.arange(n):\r\n","        r, g, b = np.zeros(3)\r\n","\r\n","        for j in np.arange(8):\r\n","            r = r + (1<<(7-j))*((i&(1<<(3*j))) >> (3*j))\r\n","            g = g + (1<<(7-j))*((i&(1<<(3*j+1))) >> (3*j+1))\r\n","            b = b + (1<<(7-j))*((i&(1<<(3*j+2))) >> (3*j+2))\r\n","\r\n","        cmap[i,:] = np.array([r, g, b])\r\n","\r\n","    return cmap\r\n","\r\n","class Relabel:\r\n","\r\n","    def __init__(self, olabel, nlabel):\r\n","        self.olabel = olabel\r\n","        self.nlabel = nlabel\r\n","\r\n","    def __call__(self, tensor):\r\n","        assert isinstance(tensor, torch.LongTensor) or isinstance(tensor, torch.ByteTensor) , 'tensor needs to be LongTensor'\r\n","        tensor[tensor == self.olabel] = self.nlabel\r\n","        return tensor\r\n","\r\n","\r\n","class ToLabel:\r\n","\r\n","    def __call__(self, image):\r\n","        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\r\n","\r\n","\r\n","class Colorize:\r\n","\r\n","    def __init__(self, n=22):\r\n","        #self.cmap = colormap(256)\r\n","        self.cmap = colormap_cityscapes(256)\r\n","        self.cmap[n] = self.cmap[-1]\r\n","        self.cmap = torch.from_numpy(self.cmap[:n])\r\n","\r\n","    def __call__(self, gray_image):\r\n","        size = gray_image.size()\r\n","        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\r\n","\r\n","        #for label in range(1, len(self.cmap)):\r\n","        for label in range(0, len(self.cmap)):\r\n","            mask = gray_image[0] == label\r\n","\r\n","            color_image[0][mask] = self.cmap[label][0]\r\n","            color_image[1][mask] = self.cmap[label][1]\r\n","            color_image[2][mask] = self.cmap[label][2]\r\n","\r\n","        return color_image"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5MNKuVrs6gRz"},"source":["# main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fr7zMh3p6h3x","executionInfo":{"status":"ok","timestamp":1614607388642,"user_tz":-540,"elapsed":3355,"user":{"displayName":"양태규","photoUrl":"","userId":"14306298094991690602"}},"outputId":"e2f2d93f-afd7-4ef8-b962-4ca880661d20"},"source":["# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \r\n","# Sept 2017\r\n","# Eduardo Romera\r\n","#######################\r\n","\r\n","import numpy as np\r\n","import torch\r\n","import os\r\n","import importlib\r\n","\r\n","from PIL import Image\r\n","from argparse import ArgumentParser\r\n","\r\n","from torch.autograd import Variable\r\n","from torch.utils.data import DataLoader\r\n","from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\r\n","from torchvision.transforms import ToTensor, ToPILImage\r\n","\r\n","NUM_CHANNELS = 3\r\n","NUM_CLASSES = 20\r\n","\r\n","image_transform = ToPILImage()\r\n","input_transform_cityscapes = Compose([\r\n","    Resize((512,1024),Image.BILINEAR),\r\n","    ToTensor(),\r\n","    #Normalize([.485, .456, .406], [.229, .224, .225]),\r\n","])\r\n","target_transform_cityscapes = Compose([\r\n","    Resize((512,1024),Image.NEAREST),\r\n","    ToLabel(),\r\n","    Relabel(255, 19),   #ignore label to 19\r\n","])\r\n","\r\n","cityscapes_trainIds2labelIds = Compose([\r\n","    Relabel(19, 255),  \r\n","    Relabel(18, 33),\r\n","    Relabel(17, 32),\r\n","    Relabel(16, 31),\r\n","    Relabel(15, 28),\r\n","    Relabel(14, 27),\r\n","    Relabel(13, 26),\r\n","    Relabel(12, 25),\r\n","    Relabel(11, 24),\r\n","    Relabel(10, 23),\r\n","    Relabel(9, 22),\r\n","    Relabel(8, 21),\r\n","    Relabel(7, 20),\r\n","    Relabel(6, 19),\r\n","    Relabel(5, 17),\r\n","    Relabel(4, 13),\r\n","    Relabel(3, 12),\r\n","    Relabel(2, 11),\r\n","    Relabel(1, 8),\r\n","    Relabel(0, 7),\r\n","    Relabel(255, 0),\r\n","    ToPILImage(),\r\n","])\r\n","\r\n","def main(args):\r\n","\r\n","    modelpath = args.loadDir + args.loadModel\r\n","    weightspath = args.loadDir + args.loadWeights\r\n","\r\n","    print (\"Loading model: \" + modelpath)\r\n","    print (\"Loading weights: \" + weightspath)\r\n","\r\n","    #Import ERFNet model from the folder\r\n","    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\r\n","    model = ERFNet(NUM_CLASSES)\r\n","  \r\n","    model = torch.nn.DataParallel(model)\r\n","    if (not args.cpu):\r\n","        model = model.cuda()\r\n","\r\n","    #model.load_state_dict(torch.load(args.state))\r\n","    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\r\n","\r\n","    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\r\n","        own_state = model.state_dict()\r\n","        for name, param in state_dict.items():\r\n","            if name not in own_state:\r\n","                 continue\r\n","            own_state[name].copy_(param)\r\n","        return model\r\n","\r\n","    model = load_my_state_dict(model, torch.load(weightspath))\r\n","    print (\"Model and weights LOADED successfully\")\r\n","\r\n","    model.eval()\r\n","\r\n","    if(not os.path.exists(args.datadir)):\r\n","        print (\"Error: datadir could not be loaded\")\r\n","\r\n","\r\n","    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset),\r\n","        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\r\n","\r\n","    # For visualizer:\r\n","    # must launch in other window \"python3.6 -m visdom.server -port 8097\"\r\n","    # and access localhost:8097 to see it\r\n","    if (args.visualize):\r\n","        vis = visdom.Visdom()\r\n","\r\n","    for step, (images, labels, filename, filenameGt) in enumerate(loader):\r\n","        if (not args.cpu):\r\n","            images = images.cuda()\r\n","            #labels = labels.cuda()\r\n","\r\n","        inputs = Variable(images)\r\n","        #targets = Variable(labels)\r\n","        with torch.no_grad():\r\n","            outputs = model(inputs)\r\n","\r\n","        label = outputs[0].max(0)[1].byte().cpu().data\r\n","        #label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\r\n","        label_color = Colorize()(label.unsqueeze(0))\r\n","\r\n","        filenameSave = \"/content/gdrive/MyDrive/save/\" + filename[0].split(\"leftImg8bit/\")[1]\r\n","        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\r\n","        #image_transform(label.byte()).save(filenameSave)      \r\n","        label_save = ToPILImage()(label_color)           \r\n","        label_save.save(filenameSave) \r\n","\r\n","        if (args.visualize):\r\n","            vis.image(label_color.numpy())\r\n","        print (step, filenameSave)\r\n","\r\n","    \r\n","\r\n","if __name__ == '__main__':\r\n","    parser = ArgumentParser()\r\n","\r\n","    parser.add_argument('--state')\r\n","\r\n","    parser.add_argument('--loadDir',default=\"/content/gdrive/MyDrive/save/\")\r\n","    parser.add_argument('--loadWeights', default=\"model_best.pth\")\r\n","    parser.add_argument('--loadModel', default=\"erfnet.py\")\r\n","    parser.add_argument('--subset', default=\"val\")  #can be val, test, train, demoSequence\r\n","\r\n","    parser.add_argument('--datadir', default=\"/content/gdrive/MyDrive/cityscapes\")\r\n","    parser.add_argument('--num-workers', type=int, default=4)\r\n","    parser.add_argument('--batch-size', type=int, default=1)\r\n","    parser.add_argument('--cpu', action='store_true', default=False)\r\n","\r\n","    parser.add_argument('--visualize', action='store_true', default=False)\r\n","    main(parser.parse_args(''))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Loading model: /content/gdrive/MyDrive/pre/erfnet.py\n","Loading weights: /content/gdrive/MyDrive/pre/erfnet_pretrained.pth\n","Model and weights LOADED successfully\n","0 /content/gdrive/MyDrive/save/val/lindau/lindau_000028_000019_leftImg8bit.png\n"],"name":"stdout"}]}]}